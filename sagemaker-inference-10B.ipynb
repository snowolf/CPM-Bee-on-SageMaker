{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ae65fec",
   "metadata": {},
   "source": [
    "# Deploy the CPM-Bee 10B  model on Amazon SageMaker\n",
    "\n",
    "As we have finetuned the model, next we will show you how to deploy the model on SageMaker.\n",
    "\n",
    "In this notebook, we explore how to host a large language model on SageMaker using the [Large Model Inference](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-large-model-inference.html) container that is optimized for hosting large models using DJLServing. DJLServing is a high-performance universal model serving solution powered by the Deep Java Library (DJL) that is programming language agnostic. To learn more about DJL and DJLServing, you can refer to our recent [blog post](https://aws.amazon.com/blogs/machine-learning/deploy-large-models-on-amazon-sagemaker-using-djlserving-and-deepspeed-model-parallel-inference/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2407531",
   "metadata": {},
   "source": [
    "## Create a SageMaker Model for Deployment\n",
    "As a first step, we'll import the relevant libraries and configure several global variables such as the hosting image that will be used nd the S3 location of our model artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5806a0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.model import Model\n",
    "from sagemaker import serializers, deserializers\n",
    "from sagemaker import image_uris\n",
    "import boto3\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import jinja2\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24862c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()  # execution role for the endpoint\n",
    "sess = sagemaker.session.Session()  # sagemaker session for interacting with different AWS APIs\n",
    "bucket = sess.default_bucket()  # bucket to house artifacts\n",
    "\n",
    "region = sess._region_name # region name of the current SageMaker Studio environment\n",
    "account_id = sess.account_id()  # account_id of the current SageMaker Studio environment\n",
    "\n",
    "s3_client = boto3.client(\"s3\") # client to intreract with S3 API\n",
    "sm_client = boto3.client(\"sagemaker\")  # client to intreract with SageMaker\n",
    "smr_client = boto3.client(\"sagemaker-runtime\") # client to intreract with SageMaker Endpoints\n",
    "jinja_env = jinja2.Environment() # jinja environment to generate model configuration templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38930529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lookup the inference image uri based on our current region\n",
    "djl_inference_image_uri = (\n",
    "    f\"763104351884.dkr.ecr.{region}.amazonaws.com/djl-inference:0.21.0-deepspeed0.8.3-cu117\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d4b349e",
   "metadata": {},
   "source": [
    "**The model we tested is from https://huggingface.co/openbmb/cpm-bee-10b, you could download first and upload to S3, you could follow below commented code to download the model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc02bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d98fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from huggingface_hub import snapshot_download\n",
    "# from pathlib import Path\n",
    "\n",
    "# local_cache_path = Path(\"./model\")\n",
    "# local_cache_path.mkdir(exist_ok=True)\n",
    "\n",
    "# model_name = \"openbmb/cpm-bee-10b\"\n",
    "\n",
    "# # Only download pytorch checkpoint files\n",
    "# allow_patterns = [\"*.json\", \"*.pt\", \"*.bin\", \"*.model\"]\n",
    "\n",
    "# model_download_path = snapshot_download(\n",
    "#     repo_id=model_name,\n",
    "#     cache_dir=local_cache_path,\n",
    "#     allow_patterns=allow_patterns,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cf471c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the model files path\n",
    "# import os\n",
    "# from glob import glob\n",
    "\n",
    "# local_model_path = None\n",
    "\n",
    "# paths = os.walk(r'./model')\n",
    "# for root, dirs, files in paths:\n",
    "#     for file in files:\n",
    "#         if file == 'config.json':\n",
    "#             print(os.path.join(root,file))\n",
    "#             local_model_path = str(os.path.join(root,file))[0:-11]\n",
    "#             print(local_model_path)\n",
    "# if local_model_path == None:\n",
    "#     print(\"Model download may failed, please check prior step!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550dcd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%script env sagemaker_default_bucket=$sagemaker_default_bucket local_model_path=$local_model_path bash\n",
    "\n",
    "# chmod +x ./s5cmd\n",
    "# ./s5cmd sync ${local_model_path} s3://${sagemaker_default_bucket}/llm/models/cpm-bee/l0B/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e265e771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # remove model artifact from local notebook storage\n",
    "# !rm -rf model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00f2f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model_location = \"s3://sagemaker-us-west-2-169088282855/llm/models/cpm-bee/10B/\"# Change to the model artifact path in S3 which we get from the fine tune job\n",
    "print(f\"Pretrained model will be downloaded from ---- > {pretrained_model_location}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e33506",
   "metadata": {},
   "source": [
    "## Deploying a Large Language Model using Hugging Face Accelerate\n",
    "The DJL Inference Image which we will be utilizing ships with a number of built-in inference handlers for a wide variety of tasks including:\n",
    "- `text-generation`\n",
    "- `question-answering`\n",
    "- `text-classification`\n",
    "- `token-classification`\n",
    "\n",
    "You can refer to this [GitRepo](https://github.com/deepjavalibrary/djl-serving/tree/master/engines/python/setup/djl_python) for a list of additional handlers and available NLP Tasks. <br>\n",
    "These handlers can be utilized as is without having to write any custom inference code. We simply need to create a `serving.properties` text file with our desired hosting options and package it up into a `tar.gz` artifact.\n",
    "\n",
    "Lets take a look at the `serving.properties` file that we'll be using for our first example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3570119",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir accelerate_src"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6cb75e",
   "metadata": {},
   "source": [
    "**IMPORTANT** The ```option.tensor_parallel_degree``` means how GPU we will use to load a single model, here we set it to 2, as 10B model need at least 20GB (fp16) GPU memory, here we use 4 GPU to host this 10B model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52b06ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile accelerate_src/requirements.txt\n",
    "transformers==4.30.2\n",
    "accelerate==0.20.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c253cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile accelerate_src/serving.template\n",
    "engine=Python\n",
    "# option.entryPoint=djl_python.huggingface\n",
    "option.s3url={{ s3url }}\n",
    "option.task=text-generation\n",
    "option.device_map=auto\n",
    "option.dtype=fp16\n",
    "option.tensor_parallel_degree=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbdefb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we plug in the appropriate model location into our `serving.properties` file based on the region in which this notebook is running\n",
    "template = jinja_env.from_string(Path(\"accelerate_src/serving.template\").open().read())\n",
    "Path(\"accelerate_src/serving.properties\").open(\"w\").write(template.render(s3url=pretrained_model_location))\n",
    "!pygmentize accelerate_src/serving.properties | cat -n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46cffa8",
   "metadata": {},
   "source": [
    "There are a few options specified here. Lets go through them in turn<br>\n",
    "1. `engine` - specifies the engine that will be used for this workload. In this case we'll be hosting a model using the [DJL Python Engine](https://github.com/deepjavalibrary/djl-serving/tree/master/engines/python)\n",
    "2. `option.entryPoint` - specifies the entrypoint code that will be used to host the model. djl_python.huggingface refers to the `huggingface.py` module from [djl_python repo](https://github.com/deepjavalibrary/djl-serving/tree/master/engines/python/setup/djl_python).  \n",
    "3. `option.s3url` - specifies the location of the model files. Alternativelly an `option.model_id` option can be used instead to specifiy a model from Hugging Face Hub (e.g. `EleutherAI/gpt-j-6B`) and the model will be automatically downloaded from the Hub. The s3url approach is recommended as it allows you to host the model artifact within your own environment and enables faster deployments by utilizing optimized approach within the DJL inference container to transfer the model from S3 into the hosting instance \n",
    "4. `option.task` - This is specific to the `huggingface.py` inference handler and specifies for which task this model will be used\n",
    "5. `option.device_map` - Enables layer-wise model partitioning through [Hugging Face Accelerate](https://huggingface.co/docs/accelerate/usage_guides/big_modeling#designing-a-device-map). With `option.device_map=auto`, Accelerate will determine where to put each **layer** to maximize the use of your fastest devices (GPUs) and offload the rest on the CPU, or even the hard drive if you don’t have enough GPU RAM (or CPU RAM). Even if the model is split across several devices, it will run as you would normally expect.\n",
    "\n",
    "For more information on the available options, please refer to the [SageMaker Large Model Inference Documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/realtime-endpoints-large-model-configuration.html)\n",
    "\n",
    "Our initial approach here is to utilize the built-in functionality within Hugging Face Transformers to enable Large Language Model hosting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d156470a",
   "metadata": {},
   "source": [
    "We place the `serving.properties` file into a tarball and upload it to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d2936e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile accelerate_src/model.py\n",
    "from djl_python import Input, Output\n",
    "import os\n",
    "import logging\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from accelerate import dispatch_model\n",
    "\n",
    "model = None\n",
    "tokenizer = None\n",
    "\n",
    "def load_model(properties):\n",
    "    tensor_parallel = properties[\"tensor_parallel_degree\"]\n",
    "    model_location = properties['model_dir']\n",
    "    if \"model_id\" in properties:\n",
    "        model_location = properties['model_id']\n",
    "    logging.info(f\"Loading model in {model_location}\")\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_location, trust_remote_code=True)\n",
    "    \n",
    "    #for HF accelerate inference\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(model_location, trust_remote_code=True)\n",
    "    device_map = {\n",
    "        \"cpmbee.input_embedding\": 0,\n",
    "        \"cpmbee.position_bias\": 0,\n",
    "        \"lm_head\": 0,\n",
    "        \"cpmbee.encoder.output_layernorm\": 0\n",
    "    }\n",
    "\n",
    "    for i in range(48):\n",
    "        device_map[\"cpmbee.encoder.layers.{}\".format(i)] = i % 4\n",
    "\n",
    "    model = dispatch_model(model, device_map=device_map)\n",
    "\n",
    "    return model, tokenizer\n",
    "\n",
    "\n",
    "def handle(inputs: Input) -> None:\n",
    "    global model, tokenizer\n",
    "    try:\n",
    "        if not model:\n",
    "            model,tokenizer = load_model(inputs.get_properties())\n",
    "\n",
    "        #print(inputs)\n",
    "        if inputs.is_empty():\n",
    "            # Model server makes an empty call to warmup the model on startup\n",
    "            return None\n",
    "        \n",
    "        if inputs.is_batch():\n",
    "            #the demo code is just suitable for single sample per client request\n",
    "            bs = inputs.get_batch_size()\n",
    "            logging.info(f\"Dynamic batching size: {bs}.\")\n",
    "            batch = inputs.get_batches()\n",
    "            #print(batch)\n",
    "            tmp_inputs = []\n",
    "            for _, item in enumerate(batch):\n",
    "                tmp_item = item.get_as_json()\n",
    "                tmp_inputs.append(tmp_item.get(\"inputs\"))\n",
    "            \n",
    "            #For server side batch, we just use the custom generation parameters for single Sagemaker Endpoint.\n",
    "            result = model.generate(tmp_inputs, tokenizer)\n",
    "            \n",
    "            outputs = Output()\n",
    "            for i in range(len(result)):\n",
    "                outputs.add(result[i], key=\"generate_text\", batch_index=i)\n",
    "            return outputs\n",
    "        else:\n",
    "            inputs = inputs.get_as_json()\n",
    "            if not inputs.get(\"inputs\"):\n",
    "                return Output().add_as_json({\"code\":-1,\"msg\":\"input field can't be null\"})\n",
    "\n",
    "            #input data\n",
    "            data = inputs.get(\"inputs\")\n",
    "            params = inputs.get(\"parameters\",{})\n",
    "\n",
    "            #for pure client side batch\n",
    "            if type(data) == str:\n",
    "                bs = 1\n",
    "            elif type(data) == list:\n",
    "                bs = len(data)\n",
    "            else:\n",
    "                return Output().add_as_json({\"code\":-1,\"msg\": \"input has wrong type\"})\n",
    "                \n",
    "            print(\"client side batch size is \", bs)\n",
    "            #predictor\n",
    "            result = model.generate(data, tokenizer, **params)\n",
    "\n",
    "            #return\n",
    "            return Output().add({\"code\":0,\"msg\":\"ok\",\"data\":result})\n",
    "    except Exception as e:\n",
    "        return Output().add_as_json({\"code\":-1,\"msg\":e})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12371518",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!tar czvf acc_model.tar.gz accelerate_src/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3098668f",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_code_prefix = \"cpm-bee/deploy/code/10B\"\n",
    "\n",
    "code_artifact = sess.upload_data(\"acc_model.tar.gz\", bucket, s3_code_prefix)\n",
    "print(f\"S3 Code or Model tar ball uploaded to --- > {code_artifact}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0807c58",
   "metadata": {},
   "source": [
    "## Deploy Model to a SageMaker Endpoint\n",
    "With a helper function we can now deploy our endpoint and invoke it with some sample inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26dd42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deploy_model(image_uri, model_data, role, endpoint_name, instance_type, sagemaker_session):\n",
    "    \n",
    "    \"\"\"Helper function to create the SageMaker Endpoint resources and return a predictor\"\"\"\n",
    "    model = Model(\n",
    "            image_uri=image_uri, \n",
    "              model_data=model_data, \n",
    "              role=role\n",
    "             )\n",
    "    \n",
    "    model.deploy(\n",
    "        initial_instance_count=1,\n",
    "        instance_type=instance_type,\n",
    "        endpoint_name=endpoint_name,\n",
    "#         model_data_download_timeout=60*15, ##\n",
    "        container_startup_health_check_timeout=60*15 ##\n",
    "        )\n",
    "    \n",
    "    # our requests and responses will be in json format so we specify the serializer and the deserializer\n",
    "    predictor = sagemaker.Predictor(\n",
    "        endpoint_name=endpoint_name, \n",
    "        sagemaker_session=sagemaker_session, \n",
    "        serializer=serializers.JSONSerializer(), \n",
    "        deserializer=deserializers.JSONDeserializer())\n",
    "    \n",
    "    return predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75889a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# creates a unique endpoint name\n",
    "endpoint_name = sagemaker.utils.name_from_base(\"cpm-bee-10B\")\n",
    "print(f\"Our endpoint will be called {endpoint_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bf4ee4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# deployment will take about 10 minutes\n",
    "predictor = deploy_model(image_uri=djl_inference_image_uri, \n",
    "                            model_data=code_artifact, \n",
    "                            role=role, \n",
    "                            endpoint_name=endpoint_name, \n",
    "                            instance_type=\"ml.g5.12xlarge\", \n",
    "                            sagemaker_session=sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c19a0c",
   "metadata": {},
   "source": [
    "Let's run an example with a basic text generation prompt Large model inference is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43611bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "data_list = [\n",
    "    {\"input\": \"今天天气是真的\", \"prompt\": \"往后写两句话\", \"<ans>\": \"\"},\n",
    "    {\"input\": \"北京市气象台提示，4月12日午后偏南风加大，阵风可达6级左右，南下的沙尘可能伴随回流北上进京，外出仍需注意<mask_0>，做好健康防护。天津市气象台也提示，受<mask_1>影响，我市4月12日有浮尘天气，PM10浓度<mask_2>。请注意关好门窗，老人儿童尽量减少户外活动，外出注意带好<mask_3>。” \",\"<ans>\":{\"<mask_0>\":\"\",\"<mask_1>\":\"\",\"<mask_2>\":\"\",\"<mask_3>\":\"\"}},\n",
    "]\n",
    "\n",
    "result = predictor.predict({ \n",
    "                    \"inputs\" : data_list, \n",
    "                    \"parameters\": {\"max_new_tokens\": 50, \"repetition_penalty\": 1.1, \"temperature\": 0.5}\n",
    "                })\n",
    "\n",
    "for res in result[\"data\"]:\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8dc03d3",
   "metadata": {},
   "source": [
    "#### Clean up endpoint to save cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2da0c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the endpoint before proceeding\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7377c84c",
   "metadata": {},
   "source": [
    "## Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a842b9",
   "metadata": {},
   "source": [
    "[sagemaker-hosting/Large-Language-Model-Hosting/](https://github.com/aws-samples/sagemaker-hosting/tree/main/Large-Language-Model-Hosting)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
